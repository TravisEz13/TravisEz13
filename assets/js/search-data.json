{
  
    
        "post0": {
            "title": "Using DotNet-Interactive to render a scatter graph",
            "content": "Sierpinski&#39;s Gasket in DotNet-Interactive using Powershell . My goal is just to demonstrate how to use a scatter plot. . Initialization . I&#39;ll define the following: . size of the axis | number of points I&#39;ll generate | two arrays to hold the coordinates of the points | x-axis | y-axis | a marker use for plotting | the class I&#39;ll use to track the current point | an instance tracking the current point | . #Define the x and y axis max $max = 100 #Define the number of points at which we will stop and render the chart $pointCount = 20000 # Create the list of x and y location that the point landed on $x = [System.Collections.Generic.List[Double]]::new($pointCount) $y = [System.Collections.Generic.List[Double]]::new($pointCount) # Define the x-axis for the plot $xaxis = [XPlot.Plotly.Graph+Xaxis]::new() $xaxis.showticklabels = $false $xaxis.showgrid = $false $xaxis.zeroline = $false # Define the y-axis for the plot $yaxis = [XPlot.Plotly.Graph+Yaxis]::new() $yaxis.showticklabels = $false $yaxis.showgrid = $false $yaxis.zeroline = $false # Define the marker for a point $marker = [XPlot.Plotly.Graph+Marker]::new() $marker.color = &quot;rgb(34, 139, 37)&quot; $marker.Size = 2 # Define a class to track the point class point { [double] $x=0 [double] $y=0 } # Create the current point [point] $point = [point]::new() . The below script needs to be able to find the current output cell; this is an easy method to get it. Create the points . For sake of demo&#39;ing this, well measure how long everything takes. . Well create a loop for the number of points we want to create. In the loop, we will create a direction with 3 possible values, If it&#39;s the first, we will move the point half way towards 0, 0. If it&#39;s the second, we will move the point half way towards, max,0. If it&#39;s the third, we will move the point half way towards, 0,max. Then, add the point to the arrays, and loop. . When done with the loop, we will print the amount of time this process took. . # Create a look for the number of points to create $seconds = (Measure-Command { foreach($i in 1..$pointCount) { # Get and random number to choose which direction to move $direction = Get-random -Minimum 0 -Maximum 3 switch($direction){ # if 0, move toward 0,0 0{ Write-Verbose &quot;towards 0&quot; $point.x= $point.x/2 $point.y = $point.y/2 } # if 1, move toward max,0 1{ Write-Verbose &quot;towards xMax&quot; $point.x= ($point.x+$max)/2 $point.y = $point.y/2 } # if 2, move toward 0,max 2{ Write-Verbose &quot;towards yMax&quot; $point.x= $point.x/2 $point.y = ($point.y+$max)/2 } } # add the current point to the lists $x.Add($point.x) $y.Add($point.y) } }).TotalSeconds Write-Verbose -Message &quot;data creation seconds: $seconds for $pointCount points&quot; -Verbose . VERBOSE: data creation seconds: 3.035501 for 20000 points . Plotting the points . First, create a scattergl trace. Then, assing the x and y arrays of points coordinates to the traces properties. Then, set the mode to markers Then, set the marker to the marker we initialized earlier. . I&#39;ll create a layout to make the plot a little bigger (550x550 is about the largest I could get it to reliably render in jupyter-lab.) Then, assign the y and x axis we created earlier to the apporiate properties. Then, use New-PlotlyChart to create the chart and Out-Display to render it. . Again, I print the time this took. . # Create the trace of the points $seconds = (Measure-Command { $p1 = [XPlot.Plotly.Graph+Scattergl]::new() $p1.x = $x.ToArray() $p1.y = $y.ToArray() $p1.mode = &quot;markers&quot; $p1.marker = $marker $layout = [XPlot.Plotly.Layout+Layout]::new() $layout.width = 550 $layout.height = 550 $layout.xaxis = $xaxis $layout.yaxis = $yaxis # Render the chart New-PlotlyChart -trace $p1 -Layout $layout | Out-Display }).TotalSeconds Write-Verbose -Message &quot;rendering seconds: $seconds&quot; -Verbose . VERBOSE: rendering seconds: 0.2375148 .",
            "url": "http://fastpages.ez13.net/2020/05/06/sierpinski.html",
            "relUrl": "/2020/05/06/sierpinski.html",
            "date": " • May 6, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "PowerShell Core Release Improvements",
            "content": "Overview . For PowerShell Core, we basically had to build a new engineering system to build and release it. How we build it has evolved over time as we learn and our other teams have implemented features that make some tasks easier. We are finally at a state that we believe we can engineer a system that builds PowerShell Core for release with as little human interaction as necessary. . Current state . Before the changes described here, we had one build per platform. After the binaries were built, they had to be tested and then packaged into the various packages for release. This is done in a private Azure DevOps Pipelines instance. In this state, it took a good deal of people’s time to do a PowerShell Core release. Before these changes, it would take 3-4 people about a week to release PowerShell Core. During this time, the percentage of time people were focused on the release probably averaged 50%. . Goals . Remain compliant with Microsoft and external standards we are required to follow. | Automate as much of the build, test, and release process as possible. This should significantly reduce the amount of human toil needed in each release. | . | Hopefully, provide some tools or practices others can follow. | What we have done so far . We ported our CI tests to Azure DevOps Pipelines. We have used this in a release and we see that this allowed us to run at least those test in our private Azure DevOps Pipelines instance. | This saves us 2-4 man hours per release and a day or more of calendar time if all goes well. | . | We have moved our release build definitions to YAML. We have used this in a release and we see that this allows us to treat the release build as code and iterate more quickly. | This saves us 1-2 man hours per release, when we have done everything correctly. | . | I have begun to merge the the different platform builds into one combined build. We have not yet used this in a release but we believe this should allow us to have a single button that gets us ready to test. | This has not been in use long enough to determine how much time it will save. | . | We have begun to automate our releases testing. Our release testing is very similar to our CI testing just across more distributions and versions of Windows. We plan to be able to run this through Azure DevOps Pipelines as well. This has not been in use long enough to determine how much time it will save. | . | We have automated the generation of the draft of the change log and categorizing the entries based on labels the maintainers apply to the PRs. After generation, the maintainers still need to review the change descriptions to make sure it makes sense in the change log. This saves us 2-4 man hours per release. | . | Summary of improvements . After all these changes, we can now release with 2-3 people in 2 to 3 days, with an average of 25% time focusing on the release. . Details of the combined build . Azure DevOps Pipelines allows us to define complex build pipeline. The build will be complex but things like templates in Azure DevOps makes breaking it into a manageable pieces. . Although this design does not technically reduce the number of parts, one significant thing it does for us it put all of our artifacts, in one place. Having the artifacts in one place, reduces the input to the steps in the rest of the build such as test and release. . I’m not going to discuss it much, but in order to coordinate this work we are keeping diagram of the build. I’ll include it here. If you want me to post another blog on the details, please leave a comment. . . What is left to do . We still have to add the other various NuGet package build steps to the coordinated build. | We need to automate functionality (CI tests) across a representative sample of supported platforms. | It would be nice if we could enforce in GitHub the process that helps us automate the change log generation. | We need to automate the release process including: Automating package testing. For example, MSI, Zip, Deb, RPM, and Snap. | Automating the actual release to GitHub, mcr.microsoft.com, packages.microsoft.com and the Snap store. | . |",
            "url": "http://fastpages.ez13.net/powershell/2019/04/09/powershell-release-improvements.html",
            "relUrl": "/powershell/2019/04/09/powershell-release-improvements.html",
            "date": " • Apr 9, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Decrypting the current MOF on WMF5",
            "content": "At the PowerShell Summit in Bellevue, I presented about how DSC now automatically encrypts the current.mof, in order to address some customer concerns about the existing encryption. After this talk, I was asked some questions about how to decrypt this new encryption. When I got back to work, I added a simple function, Unprotect-xDscConfiguration to the xDscDiagnostics module on GitHub. It takes a parameter for the name of the stage that you would like to decrypt (see the documention for the stage parameter of Remove-DSCConfigurationDocument). The function will currently only work locally (feel free to submit an issue or a PR) and must be run as administrator to be able to decrypt the MOF. Example usage and output are below as well. . Installation . At the time of the writing, the release version (2.2.0.0) of xDscDiagnostics dose not have this change, but there are instruction on how to install the development version using PowerShell Get on GitHub. . Example Usage . Unprotect-xDscConfigurtion -Stage Previous . Example output . /* @TargetNode=&#39;localhost&#39; @GeneratedBy=tplunk @GenerationDate=04/07/2016 16:54:16 @GenerationHost=localhost */ instance of MSFT_LogResource as $MSFT_LogResource1ref { SourceInfo = &quot;::1::24::log&quot;; ModuleName = &quot;PsDesiredStateConfiguration&quot;; ResourceID = &quot;[Log]example&quot;; Message = &quot;example&quot;; ModuleVersion = &quot;1.0&quot;; ConfigurationName = &quot;example&quot;; }; instance of OMI_ConfigurationDocument { Version=&quot;2.0.0&quot;; MinimumCompatibleVersion = &quot;1.0.0&quot;; CompatibleVersionAdditionalProperties= {&quot;Omi_BaseResource:ConfigurationName&quot;}; Author=&quot;tplunk&quot;; GenerationDate=&quot;04/07/2016 16:54:16&quot;; GenerationHost=&quot;localhost&quot;; Name=&quot;example&quot;; }; .",
            "url": "http://fastpages.ez13.net/dsc/2016/04/19/decrypt-dsc-mof.html",
            "relUrl": "/dsc/2016/04/19/decrypt-dsc-mof.html",
            "date": " • Apr 19, 2016"
        }
        
    
  
    
        ,"post3": {
            "title": "How to map downloads of the extension dependencies to your own location.",
            "content": "Originally Posted on MSDN. . NOTE: For information on OS support, and other features, please refer to our release history. . Overview . Today, we released version 2.8 of the Azure DSC Extension, we added support to map downloads of the extension dependencies to your own location. This could be useful, if you want to configure the network, of a VM not to allow direct access to the Internet and host these files somewhere else. This blog will show you how to use this feature in Azure Cloud Service Manager (ASM). This assumes you already know how to use the DSC Extension as described in How to use WMF 4 with Azure DSC Extension in Azure Resource Manager (ARM) and How to use WMF 4 with Azure DSC Extension in Azure Cloud Service Manager (ASM). We are working to add this feature into the Azure PowerShell SDK DSC Extension Cmdlets directly. Currently the Download Mapping feature is only available if you form the JSON yourself and send it to the extension using the generic extension Cmdlet (Set-AzureVMExtension.) . In this example I will show you: . How to update the function from the previous ASM example to add the download mappings JSON you need to send to the extension. | How to update the JSON from the previous ARM example to add the download mapping you need to send to the extension. | Adding the Download Mappings JSON . In the New-XAzureVmDscExtensionJson from the previous ASM blog, add the download mapping parameter with the type of HashTable. Additionally, we will update the wmfVersion parameter to not be required and set the default value to make the function easier to use. . [ValidateNotNullOrEmpty()] [ValidateSet(&#39;4.0&#39;,&#39;latest&#39;,&#39;5.0PP&#39;)] [string] $WmfVersion = &#39;latest&#39;, [AllowNull()] [hashtable] $DownloadMappings . Here is an explanation of the values for download mapping: . The key of the hash table: Should be the key for the download in the following format: &lt;PKG&gt;_&lt;PKGVersion&gt;-&lt;Platform&gt;_&lt;PlatformVersion&gt;-&lt;Arch&gt; PKG WMF - for WMF Packages | NET - for .NET packages | . | PKGVerion The Version of the Package. Currently WMF has two valid versions, 4.0 or 5.0PP (latest translates to 5.0PP.) .NET only has one valid version currently, 4.5. | . | Platform Reserved for future use, currently always `Windows’ | . | `PlatformVersion’ Windows Server 2008 R2 - 6.1 | Windows Server 2012 - 6.2 | Windows Server 2012 R2 - 6.3 | Windows Server 2016 Technical Preview - 10.0 | . | Arch AMD64/x64 - x64 | x86 - x86 | . | . | Example to map the download of WMF 4.0 for Windows Server 2008 R2 X64 to your own URL the key would be: WMF_4.0-Windows_6.1-x64 | . | . | The value of the table must be a HTTPS URL to download the package without credential (query strings are allowed.) | . This function should produce a JSON that looks something like this. . &quot;Properties&quot;: { &quot;DestinationPath&quot;: &quot;C: test&quot; }, &quot;advancedOptions&quot;: { &quot;DownloadMappings&quot;: { &quot;WMF_4.0-Windows_6.1-x64&quot;: &quot;https://mystorage.blob.core.windows.net/mypubliccontainer/Windows6.1-KB2819745-x64-MultiPkg.msu&quot; } }, &quot;WmfVersion&quot;: &quot;latest&quot;, &quot;ConfigurationFunction&quot;: &quot;configuration.ps1 ConfigurationName&quot;, &quot;ModulesUrl&quot;: &quot;https://storageaccountname.blob.core.windows.net/windows-powershell-dsc/configuration.ps1.zip?&lt;sastoken&gt;&quot; } . Putting it all together and Sending it to a VM . We put it all together the same as last time, just passing the HashTable with download key and the URL to map the download to. The following is an example of how to do that. . $storageAccountName = &#39;storageaccountname&#39; $publisher = &#39;Microsoft.Powershell&#39; $dscVersion = &#39;2.8&#39; $serviceName = &#39;servicename&#39; $vmName = &#39;vmName&#39; $moduleName = &#39;configuration.ps1&#39; $blobName = &quot;$moduleName.zip&quot; $configurationPath = &quot;$PSScriptRoot $moduleName&quot; $ConfigurationName = &#39;ConfigurationName&#39; $modulesUrl = Get-XAzureDscPublishedModulesUrl -blobName $blobName -configurationPath $configurationPath ` -storageAccountName $storageAccountName Write-Verbose -Message &quot;ModulesUrl: $modulesUrl&quot; -Verbose $PublicConfigurationJson = New-XAzureVmDscExtensionJson -moduleName $moduleName -modulesUrl $modulesUrl ` -properties @{DestinationPath = &#39;C: test&#39;} -configurationName $ConfigurationName -DownloadMappings @{&#39;WMF_4.0-Windows_6.1-x64&#39; = &#39;https://mystorage.blob.core.windows.net/mypubliccontainer/Windows6.1-KB2819745-x64-MultiPkg.msu&#39;} Write-Verbose -Message &quot;PublicConfigurationJson: $PublicConfigurationJson&quot; -Verbose $vm = get-azurevm -ServiceName $serviceName -Name $vmName $vm = Set-AzureVMExtension ` -VM $vm ` -Publisher $publisher ` -ExtensionName &#39;DSC&#39; ` -Version $dscVersion ` -PublicConfiguration $PublicConfigurationJson ` -ForceUpdate $vm | Update-AzureVM . After the VM is finished update, you should have a VM that used the URL you specified to download the WMF needed to install the extension. I’ll have a follow-up blog on how to do this in ARM. . I have published these samples to GitHub as a working script. Just update the, Service Name, etc. and run the script in the Azure PowerShell SDK. . Adding the download mappings to the ARM JSON . In the previous ARM blog, I described how to use visual studio to create an ARM Template to select which WMF the extension used. I will start with the JSON that we ended with in that blog. . { &quot;name&quot;: &quot;Microsoft.Powershell.DSC&quot;, &quot;type&quot;: &quot;extensions&quot;, &quot;location&quot;: &quot;[variables(&#39;location&#39;)]&quot;, &quot;apiVersion&quot;: &quot;2015-05-01-preview&quot;, &quot;dependsOn&quot;: [ &quot;[concat(&#39;Microsoft.Compute/virtualMachines/&#39;, variables(&#39;vmName&#39;))]&quot; ], &quot;tags&quot;: { &quot;displayName&quot;: &quot;Microsoft.Powershell.DSC&quot; }, &quot;properties&quot;: { &quot;publisher&quot;: &quot;Microsoft.Powershell&quot;, &quot;type&quot;: &quot;DSC&quot;, &quot;typeHandlerVersion&quot;: &quot;2.1&quot;, &quot;autoUpgradeMinorVersion&quot;: true, &quot;settings&quot;: { &quot;modulesUrl&quot;: &quot;[concat(parameters(&#39;_artifactsLocation&#39;), &#39;/&#39;, &#39;dsc.zip&#39;)]&quot;, &quot;sasToken&quot;: &quot;[parameters(&#39;_artifactsLocationSasToken&#39;)]&quot;, &quot;configurationFunction&quot;: &quot;[variables(&#39;Microsoft.Powershell.DSCConfigurationFunction&#39;)]&quot;, &quot;wmfVersion&quot;: &quot;4.0&quot;, &quot;properties&quot;: { &quot;nodeName&quot;: &quot;[variables(&#39;vmName&#39;)]&quot; } }, &quot;protectedSettings&quot;: { } } } . To that we will add the following JSON after modulesUrl. . &quot;advancedOptions&quot;: { &quot;DownloadMappings&quot;: { &quot;WMF_4.0-Windows_6.1-x64&quot;: &quot;https://mystorage.blob.core.windows.net/mypubliccontainer/Windows6.1-KB2819745-x64-MultiPkg.msu&quot; } }, . This will result in the following JSON. . { &quot;name&quot;: &quot;Microsoft.Powershell.DSC&quot;, &quot;type&quot;: &quot;extensions&quot;, &quot;location&quot;: &quot;[variables(&#39;location&#39;)]&quot;, &quot;apiVersion&quot;: &quot;2015-05-01-preview&quot;, &quot;dependsOn&quot;: [ &quot;[concat(&#39;Microsoft.Compute/virtualMachines/&#39;, variables(&#39;vmName&#39;))]&quot; ], &quot;tags&quot;: { &quot;displayName&quot;: &quot;Microsoft.Powershell.DSC&quot; }, &quot;properties&quot;: { &quot;publisher&quot;: &quot;Microsoft.Powershell&quot;, &quot;type&quot;: &quot;DSC&quot;, &quot;typeHandlerVersion&quot;: &quot;2.1&quot;, &quot;autoUpgradeMinorVersion&quot;: true, &quot;settings&quot;: { &quot;modulesUrl&quot;: &quot;[concat(parameters(&#39;_artifactsLocation&#39;), &#39;/&#39;, &#39;dsc.zip&#39;)]&quot;, &quot;advancedOptions&quot;: { &quot;DownloadMappings&quot;: { &quot;WMF_4.0-Windows_6.1-x64&quot;: &quot;https://mystorage.blob.core.windows.net/mypubliccontainer/Windows6.1-KB2819745-x64-MultiPkg.msu&quot; } }, &quot;sasToken&quot;: &quot;[parameters(&#39;_artifactsLocationSasToken&#39;)]&quot;, &quot;configurationFunction&quot;: &quot;[variables(&#39;Microsoft.Powershell.DSCConfigurationFunction&#39;)]&quot;, &quot;wmfVersion&quot;: &quot;4.0&quot;, &quot;properties&quot;: { &quot;nodeName&quot;: &quot;[variables(&#39;vmName&#39;)]&quot; } }, &quot;protectedSettings&quot;: { } } } . To deploy, follow the instruction in the previous blog. I’ve put the updated solution I built on GitHub. . Feedback . Please feel free to reach to us posting comments to this post, or by posting feedback on Connect. .",
            "url": "http://fastpages.ez13.net/dsc/2015/10/21/map-dsc-ext-downloads.html",
            "relUrl": "/dsc/2015/10/21/map-dsc-ext-downloads.html",
            "date": " • Oct 21, 2015"
        }
        
    
  
    
        ,"post4": {
            "title": "How to use WMF 4 with Azure DSC Extension in Azure Resource Manager (ARM)",
            "content": "Originally posted on MSDN. . Overview . In version 2.7 of the Azure DSC Extension, we added support to leave your Virtual Machine on the latest supported version of WMF 4.0. This blog will show you how to use this feature in Azure Resource Manager (ARM) templates. For this, I will use the Azure Resource Manger Tools, that were released with Azure SDK 2.6 for .NET. I will assume you already have Visual Studio 2015 setup. I also assume you have read ‘How to use WMF 4 with Azure DSC Extension in Azure Cloud Service Manager (ASM)’, as it describes the meaning of some of the terms. The MSDN topic ‘Creating and Deploying Azure Resource Group Deployment Projects’ has a general walk-though of using the tools I will use in this blog. I would suggest you read though and refer back to this page if you loose you way while . In this Example I will show you: . How to setup the SDK for .NET, which will add the tools to Visual Studio 2015 to design and deploy an ARM Template. | How to create an ARM project in Visual Studio | How to add DSC to the ARM template | How to send this JSON to the extension on an existing VM. | Setup Azure SDK for .NET . If you haven’t already, download and install the latest Azure SDK for .NET. At the time I wrote this, the current download link for the Azure SDK for .NET was here. The current links to download the SDK can be found here. . Create a ARM Project . See the MSDN topic ‘Creating and Deploying Azure Resource Group Deployment Projects’ on how to ‘Create Azure Resource Group projects’. . Add Powershell DSC Extension . Once you have created you project, you need to add the ‘Powershell DSC Extension’ to the ARM Template. . In Solution Explorer, expand templates | Click WindowsVirtualMachine.json | Open the JSON Outline (this is usually on the left hand pane) | In the JSON Outline, expand resources | In the JSON Outline, right click Virtual Machine | In the context menu, click Add New Resource | In the Add Resource Window, click Powershell DSC Extension | In the Add Resource Window, in the Name field, type Microsoft.Powershell.DSC (Note, this is important for the SDK to work properly.) | In the Add Resource Window, click Add | This should add a section of JSON which looks like this: . { &quot;name&quot;: &quot;Microsoft.Powershell.DSC&quot;, &quot;type&quot;: &quot;extensions&quot;, &quot;location&quot;: &quot;[variables(&#39;location&#39;)]&quot;, &quot;apiVersion&quot;: &quot;2015-05-01-preview&quot;, &quot;dependsOn&quot;: [ &quot;[concat(&#39;Microsoft.Compute/virtualMachines/&#39;, variables(&#39;vmName&#39;))]&quot; ], &quot;tags&quot;: { &quot;displayName&quot;: &quot;Microsoft.Powershell.DSC&quot; }, &quot;properties&quot;: { &quot;publisher&quot;: &quot;Microsoft.Powershell&quot;, &quot;type&quot;: &quot;DSC&quot;, &quot;typeHandlerVersion&quot;: &quot;2.1&quot;, &quot;autoUpgradeMinorVersion&quot;: true, &quot;settings&quot;: { &quot;modulesUrl&quot;: &quot;[concat(parameters(&#39;_artifactsLocation&#39;), &#39;/&#39;, &#39;dsc.zip&#39;)]&quot;, &quot;sasToken&quot;: &quot;[parameters(&#39;_artifactsLocationSasToken&#39;)]&quot;, &quot;configurationFunction&quot;: &quot;[variables(&#39;Microsoft.Powershell.DSCConfigurationFunction&#39;)]&quot;, &quot;properties&quot;: { &quot;nodeName&quot;: &quot;[variables(&#39;vmName&#39;)]&quot; } }, &quot;protectedSettings&quot;: { } } } . Using the new WMF Version feature . Updating the Extension version . You must make sure you use at least the 2.7 version of the extension. In the DSC Extension JSON update &quot;typeHandlerVersion&quot;: &quot;2.1&quot; to &quot;typeHandlerVersion&quot;: &quot;2.7&quot;. This will update the DSC Extension version ARM installs from 2.1 to 2.7. . Adding the property to configure the WMF Version . You must tell the DSC Extension what version of the WMF you want to use. If you don’t it will use the latest. In the settings section add a wmfVersion property with the value 4.0. Here is an example &quot;wmfVersion&quot;: &quot;[parameters(&#39;wmfVersion&#39;)]&quot;. . After this the JSON should look like this: . { &quot;name&quot;: &quot;Microsoft.Powershell.DSC&quot;, &quot;type&quot;: &quot;extensions&quot;, &quot;location&quot;: &quot;[variables(&#39;location&#39;)]&quot;, &quot;apiVersion&quot;: &quot;2015-05-01-preview&quot;, &quot;dependsOn&quot;: [ &quot;[concat(&#39;Microsoft.Compute/virtualMachines/&#39;, variables(&#39;vmName&#39;))]&quot; ], &quot;tags&quot;: { &quot;displayName&quot;: &quot;Microsoft.Powershell.DSC&quot; }, &quot;properties&quot;: { &quot;publisher&quot;: &quot;Microsoft.Powershell&quot;, &quot;type&quot;: &quot;DSC&quot;, &quot;typeHandlerVersion&quot;: &quot;2.1&quot;, &quot;autoUpgradeMinorVersion&quot;: true, &quot;settings&quot;: { &quot;modulesUrl&quot;: &quot;[concat(parameters(&#39;_artifactsLocation&#39;), &#39;/&#39;, &#39;dsc.zip&#39;)]&quot;, &quot;sasToken&quot;: &quot;[parameters(&#39;_artifactsLocationSasToken&#39;)]&quot;, &quot;configurationFunction&quot;: &quot;[variables(&#39;Microsoft.Powershell.DSCConfigurationFunction&#39;)]&quot;, &quot;wmfVersion&quot;: &quot;4.0&quot;, &quot;properties&quot;: { &quot;nodeName&quot;: &quot;[variables(&#39;vmName&#39;)]&quot; } }, &quot;protectedSettings&quot;: { } } } . Deploying your configuration . The solution should include a Microsoft.Powershell.DSCConfiguration.ps1 which it will deploy using the DSC Extension. . See the MSDN topic ‘Creating and Deploying Azure Resource Group Deployment Projects’ on ‘Deploying an Azure Resource Group project to an Azure resource group’. . For a more detailed description of how to deploy see the blog “Deploying a Website with Content through Visual Studio with Resource Groups” . Summary . This should be enough to let you choose which version of WMF you want to use with ARM. I’ve put the solution I built on GitHub and turned the WmfVersion into a parameter on the template. This will let you choose what version you want at deployment time. I also modified the default DSC configuration to report the WMF major Version so it’s easy to verify it is working without logging into the machine. . Notes . Windows Server 2016 Technical Preview has the equivalent of WMF 5 already installed. Therefore, specifying WMF 4 for this OS is not a valid option. . References . If you want to dive in further on ARM concepts, Greg Oliver has written the blog “Developing DSC scripts for the Azure Resource Manager DSC Extension”. .",
            "url": "http://fastpages.ez13.net/dsc/2015/10/02/WMF-4-dsc-ext-arm.html",
            "relUrl": "/dsc/2015/10/02/WMF-4-dsc-ext-arm.html",
            "date": " • Oct 2, 2015"
        }
        
    
  
    
        ,"post5": {
            "title": "How to use WMF 4 with Azure DSC Extension in Azure Cloud Service Manager (ASM)",
            "content": "Originally posted on MSDN. . Overview . In version 2.7 of the Azure DSC Extension, we added support to leave your Virtual Machine on the latest supported version of WMF 4.0. This blog will show you how to use this feature in Azure Cloud Service Manager (ASM). This assumes you already know how to create a VM in the Azure PowerShell SDK. If you don’t please see MSDN. We are working to add this feature into the Azure Powershell SDK DSC extension Cmdlets directly. Currently the WMF 4 feature is only available if you form the JSON yourself and send it to the extension using the generic extension Cmdlet. . In this Example I will show you: . How to Create the JSON you need to send to the extension. | How to create URI to published configuration which will only let people with this URI access it, known as the `ModulesUrl’ to the extension. | How to send this JSON to the extension on an existing VM. | Creating the JSON . To do this, use the Set-AzureVMExtension Cmdlet rather than the Set-AzureVMDscExtension Cmdlet and pass the JSON using the -PublicConfiguration parameter. The following function New-XAzureVmDscExtensionJson will create the JSON needed for this example. . # Create a Json to send the the DSC VM Extension function New-XAzureVmDscExtensionJson { [CmdletBinding()] param( [Parameter(Mandatory = $true)] [ValidateNotNullOrEmpty()] [string] $moduleName, [Parameter(Mandatory = $false)] [ValidateNotNull()] [string] $modulesUrl, [AllowNull()] [HashTable] $properties, [Parameter(Mandatory = $true)] [ValidateNotNullOrEmpty()] [string] $configurationName, [Parameter(Mandatory = $true)] [ValidateNotNullOrEmpty()] [ValidateSet(&#39;4.0&#39;,&#39;latest&#39;,&#39;5.0PP&#39;)] [string] $WmfVersion ) $publicSettingsTable = @{ Properties = $properties WmfVersion = $WmfVersion } if($null -ne $modulesUrl) { $publicSettingsTable.Add(&#39;ModulesUrl&#39;,$modulesUrl) } $publicSettingsTable.Add(&#39;ConfigurationFunction&#39; , &quot;${ModuleName} ${configurationName}&quot;) return ConvertTo-Json -Depth 8 $publicSettingsTable } . Here is an explanation of the values: . modulesUrl Should be a URL to a zip file generated by Publish-AzureVmDscConfiguration | . | moduleName The extension will look for this file name inside the zip file for the configuration. | . | configurationName The extension will look for a Configuration with this function inside the file/module. | . | WmfVersion The version of WMF to upgrade or leave the machine on. Currently supported values: 4.0 indicating to upgrade to the currently supported version of WMF 4.0 if a newer version isn’t already installed. * 5.0PP indicating to upgrade to the WMF 5.0PP. * latest indicating to upgrade to the latest version of WMF. | . | . | properties A Hash-table of parameters to be passed to the configuration | . | . This function should produce a JSON that looks something like this. . &quot;Properties&quot;: { &quot;DestinationPath&quot;: &quot;C: test&quot; }, &quot;WmfVersion&quot;: &quot;4.0&quot;, &quot;ConfigurationFunction&quot;: &quot;configuration.ps1 ConfigurationName&quot;, &quot;ModulesUrl&quot;: &quot;https://storageaccountname.blob.core.windows.net/windows-powershell-dsc/configuration.ps1.zip?&lt;sastoken&gt;&quot; } . Creating the `ModulesUrl’ . To generate the JSON, we need the modulesUrl this is the URL of the published configuration, with any querystring needed to access the URL. The following function Get-XAzureDscPublishedModulesUrl will publish the configuration, create a read-only SASToken for 1 hour, and return the full URI to the blob with the SASToken. . # Publish a DSC configuration, Create a SasToken, and return the full URI with the SASToken function Get-XAzureDscPublishedModulesUrl { [CmdletBinding()] param ( [Parameter(HelpMessage=&#39;The storage container to publish the configuration to&#39;)] [ValidateNotNullOrEmpty()] [String] $StorageContainer = &#39;windows-powershell-dsc&#39;, [Parameter(Mandatory=$true, Position=0, HelpMessage=&#39;The name of the blob.&#39;)] [ValidateNotNullOrEmpty()] [String] $blobName, [Parameter(Mandatory=$true, Position=1, HelpMessage=&#39;The path to the configuration to publish&#39;)] [ValidateNotNullOrEmpty()] [String] $configurationPath, [Parameter(Mandatory=$true, Position=2, HelpMessage=&#39;The name of the storage account to publish to&#39;)] [ValidateNotNullOrEmpty()] [String] $storageAccountName ) # Get the Storage Account Context function Get-AzureDscStorageAccountContext { param( [Parameter(Mandatory=$true)] [ValidateNotNullOrEmpty()] [String] $storageAccountName ) $azureStorageAccount = Get-AzureStorageAccount -StorageAccountName $storageAccountName if(!$azureStorageAccount) { throw &#39;storage account not found&#39; } $storageAccessKey = (Get-AzureStorageKey –StorageAccountName $StorageAccountName).Primary $storageContext = New-AzureStorageContext -StorageAccountName $StorageAccountName ` -StorageAccountKey $storageAccessKey return $storageContext } $expiryTime = [DateTime]::UtcNow.AddMinutes(60) #Publish the configuration Publish-AzureVMDscConfiguration -ConfigurationPath $configurationPath -Verbose -Force ` -storageContext (Get-AzureDscStorageAccountContext -storageAccountName $storageAccountName) ` -ContainerName $StorageContainer # Create a SasToken for the Configuration return New-AzureStorageBlobSASToken -Container $StorageContainer -Blob $blobName -Permission r ` -ExpiryTime $expiryTime -Context (Get-AzureDscStorageAccountContext -storageAccountName $storageAccountName) -FullUri } . Putting it all together and Sending it to a VM . Now we need to call the function to get the modulesUrl, pass the value to the function to get the JSON along with the rest of the parameters, get our VM object, and call Set-AzureVMExtension on the VM with the JSON and the parameter to send it the the DSC extension. The following is an example of how to do that. . $storageAccountName = &#39;storageaccountname&#39; $publisher = &#39;Microsoft.Powershell&#39; $dscVersion = &#39;2.7&#39; $serviceName = &#39;servicename&#39; $vmName = &#39;vmName&#39; $moduleName = &#39;configuration.ps1&#39; $blobName = &quot;$moduleName.zip&quot; $configurationPath = &quot;$PSScriptRoot $moduleName&quot; $ConfigurationName = &#39;ConfigurationName&#39; $modulesUrl = Get-XAzureDscPublishedModulesUrl -blobName $blobName -configurationPath $configurationPath ` -storageAccountName $storageAccountName Write-Verbose -Message &quot;ModulesUrl: $modulesUrl&quot; -Verbose $PublicConfigurationJson = New-XAzureVmDscExtensionJson -moduleName $moduleName -modulesUrl $modulesUrl ` -properties @{DestinationPath = &#39;C: test&#39;} -configurationName $ConfigurationName -WmfVersion &#39;4.0&#39; -Verbose Write-Verbose -Message &quot;PublicConfigurationJson: $PublicConfigurationJson&quot; -Verbose $vm = get-azurevm -ServiceName $serviceName -Name $vmName $vm = Set-AzureVMExtension ` -VM $vm ` -Publisher $publisher ` -ExtensionName &#39;DSC&#39; ` -Version $dscVersion ` -PublicConfiguration $PublicConfigurationJson ` -ForceUpdate $vm | Update-AzureVM . After the VM is finished update, you should have a WMF 4 VM. I’ll have a follow-up blog on how to do this in ARM. . I have published these samples to GitHub as a working script. Just update the, Service Name, etc. and run the script in the Azure PowerShell SDK. . Notes . Windows Server 2016 Technical Preview has the equivalent of WMF 5 already installed. Therefore, specifying WMF 4 for this OS is not a valid option. .",
            "url": "http://fastpages.ez13.net/dsc/2015/10/01/WMF-4-dsc-ext-asm.html",
            "relUrl": "/dsc/2015/10/01/WMF-4-dsc-ext-asm.html",
            "date": " • Oct 1, 2015"
        }
        
    
  
    
        ,"post6": {
            "title": "Want to secure credentials in Windows PowerShell Desired State Configuration?",
            "content": "Introduction . As you start using Windows PowerShell Desired State Configuration (DSC), you might need to specify credentials for resources. In a previous post we showed you how to define a resource that has a credential property. In this post, I’ll discuss how to properly encrypt credentials when used in a DSC configuration. . Prerequisites . First, let us discuss the requirements to encrypt a DSC configuration. . You must have an Encryption capable certificate on the target node in the Local Computer’s Personal Store (in PowerShell the path to the store is Cert: LocalMachine My, we used the workstation authentication template, see all templates here.) | If you are running the configuration, from a separate machine than the target node, you must export the public key of the certificate and import it on the machine you will be running the configuration from. It is important that you keep the private key secure. Since the public key is all that is needed to encrypt, I recommend you only export the public key to the machine you are writing your configurations on in order to keep the private key more secure. | . | . #Assumptions For this article I’m going to assume: . You are using something like Active Directory Certificate Authority to issue and distribute the encryption certificates. | Administrator access to the target node must be properly secured, as anyone with administrator access to the target node should be trusted with the credentials as the administrators can decrypt the credentials with enough effort. | . Overview . In order to encrypt credentials in a DSC configuration, you must follow a process. You must have a certificate on each target node which supports encryption. After that, you must have the public key and thumbprint of the certificate on the machine you are authoring the configuration on. The public key must be provided using the configuration data, and I’ll show you how to provide the thumbprint using configuration data as well. You must write a configuration script which configures the machine using the credentials, and sets up decryption by configuring the target node’s Local Configuration Manager (LCM) to decrypt the configuration using the encryption certificate as identified by its thumbprint. Finally, you must run the configuration, including, setting the LCM settings and starting the DSC configuration. . Configuration Data . When dealing with encryption of DSC configuration, you must understand DSC configuration data. This structure describes, to a configuration, the list of nodes to be operated on, if credentials in a configuration should be encrypted or not for each node, how credential will be encrypted, and other information you want to include. Below is an example of configuration data for a machine named targetNode, which I’d like to encrypt using a public key I’ve exported and saved to C: publicKeys targetNode.cer. . $ConfigData= @{ AllNodes = @( @{ # The name of the node we are describing NodeName = “targetNode” # The path to the .cer file containing the # public key of the Encryption Certificate # used to encrypt credentials for this node CertificateFile = “C: publicKeys targetNode.cer” # The thumbprint of the Encryption Certificate # used to decrypt the credentials on target node Thumbprint = “AC23EA3A9E291A75757A556D0B71CBBF8C4F6FD8″ }; ); } . Configuration Script . After we have the configuration data, we can start building our configuration. Since credential are important to keep secure, you should always take the credential as a parameter to your configuration. This is so the credentials are stored for the shortest time possible. Below I’ll give an example of copying a file from a share that is secured to a user. . configuration CredentialEncryptionExample { param( [Parameter(Mandatory=$true)] [ValidateNotNullorEmpty()] [PsCredential] $credential ) Node $AllNodes.NodeName { File exampleFile { SourcePath = “ Server share path file.ext” DestinationPath = “C: destinationPath” Credential = $credential } } } . When you run CredentialEncryptionExample, DSC will prompt your for the credential and encrypt the mof using the CertificateFile associated with the node in the configuration data. . Setting up Decryption . There is still one issue. When you run Start-DscConfiguration, the Local Configuration Manager (LCM) of target node does not know which certificate to use to decrypt the credentials. We need to add a LocalConfigurationManager resource to tell it. You must set the CertificateId to the thumbprint of the certificate. The first question becomes how to get the thumbprint. Below is an example of how to find a local certificate that would work for encryption (you may need to customize this to find the exact certificate you want to use.) . # Get the certificate that works for encryption function Get-LocalEncryptionCertificateThumbprint { (dir Cert: LocalMachine my) | %{ # Verify the certificate is for Encryption and valid if ($_.PrivateKey.KeyExchangeAlgorithm -and $_.Verify()) { return $_.Thumbprint } } } . After we have the thumbprint, we use this to build the configuration data (given in the above configuration data example.) Below is an example of the updated configuration with the LocalConfigurationManager resource, getting the value from the node in the configuration data. . configuration CredentialEncryptionExample { param( [Parameter(Mandatory=$true)] [ValidateNotNullorEmpty()] [PsCredential] $credential ) Node $AllNodes.NodeName { File exampleFile { SourcePath = “ Server share path file.ext” DestinationPath = “C: destinationPath” Credential = $credential } LocalConfigurationManager { CertificateId = $node.Thumbprint } } } . Running the Configuration . From this point, we need to run the configuration, it will output one *.meta.mof to configure LCM to decrypt the credentials using the certificate installed to the local machine store identified by the thumbprint, and one mof to apply the configuration. First, you will need to use Set-DscLocalConfigurationManager to apply the *.meta.mof and then, Start-DscConfiguration to apply the configuration. Here is an example of how you would run this: . Write-Host “Generate DSC Configuration…” CredentialEncryptionExample -ConfigurationData $ConfigData -OutputPath . CredentialEncryptionExample Write-Host “Setting up LCM to decrypt credentials…” Set-DscLocalConfigurationManager . CredentialEncryptionExample -Verbose Write-Host “Starting Configuration…” Start-DscConfiguration . CredentialEncryptionExample -wait -Verbose . This example would push the configuration to the target node. If you reference our blog on how to setup a pull configuration, you can modify the setting in the LocalConfigurationManager resource and use these steps to deploy this using a pull server. . Summary . You should be able to build a sample that uses credentials securely in DSC using the information in this post. I have written a more complete sample and have attached the code here: . CredentialSample.psm1 . The sample expands on what we discussed here and includes a helper cmdlet to export and copy the public keys to the local machine and an example how to use it. . Travis Plunk . Windows PowerShell DSC Test .",
            "url": "http://fastpages.ez13.net/dsc/2014/01/21/Secure-dsc-cred.html",
            "relUrl": "/dsc/2014/01/21/Secure-dsc-cred.html",
            "date": " • Jan 21, 2014"
        }
        
    
  
    
        ,"post7": {
            "title": "Installing WSUS on Windows Server “8” Beta using PowerShell",
            "content": "Originally posted on Technet. . Authors: . Travis Plunk, Software Developer Engineer | Yuri Diogenes, Senior Technical Writer | . Technical Reviewers: . Cecilia Cole, Program Manager | . In our previous post, we demonstrated how to install the WSUS Role on the Windows Server “8” Beta using the new Server Manager. In this post you will learn how to perform the same task using PowerShell. . To install WSUS on Windows Server “8” Beta using PowerShell, follow the steps below: . Sign in on Windows Server “8” Beta . | On the taskbar click Windows PowerShell button. . | In the PowerShell Console type Install-WindowsFeature -Name UpdateServices, UpdateServices-Ui and press ENTER. . | The installation process will start and a progress counter will appear on the screen. Wait until the installation reaches 100% and you receive a message that the installation succeeded before moving on to the next step. . | Type &amp; &#39;C: Program Files Update Services Tools WsusUtil.exe&#39; postinstall contentdir=C: Mycontent and press ENTER. . | Wait until you receive the message that the post installation successfully finished. . | Type Exit and press ENTER to leave the PowerShell interface. . | At this point the WSUS installation is completed and you may launch the WSUS Console using the Tools menu. When you launch WSUS for the first time the WSUS Configuration Wizard will appear. For more information about how to configure WSUS, read Step 3: Configure WSUS in the Deploying Windows Server Updates Services in the Organization article at the TechNet Library. . If you want to perform the full installation and post installation tasks using PowerShell you should run this script once you finish the installation via PowerShell. . Stay tuned for more exciting stuff on this blog and the Windows Server Blog from the server leadership team. .",
            "url": "http://fastpages.ez13.net/wsus/2012/03/20/wsus-install-win-8-preview.html",
            "relUrl": "/wsus/2012/03/20/wsus-install-win-8-preview.html",
            "date": " • Mar 20, 2012"
        }
        
    
  
    
        ,"post8": {
            "title": "Best Practices for Securing WSUS with SSL",
            "content": "Introduction . One of the questions that always come up during the planning phase of WSUS is how to secure the communication between WSUS and the clients. The general guidelines for this deployment are documented at Securing WSUS with the Secure Sockets Layer Protocol article and you should always read it first. The goal of this article is to extent this list and highlight additional considerations that you should take while planning this type of deployment. . Additional Considerations while Deploying WSUS with SSL . Use an FQDN wherever you refer to the WSUS server, including the common name used to create the SSL Certificate even on an intranet. | Require SSL so that you know your connections are secure. | Use a certificate chained to already known trusted root, issued from a certificate authority that maintains CRL (in case your certificate becomes compromised). | . Consider the Algorithm and Certificate Key length of the certificate you are using: . “The National Institute of Standards and Technology (NIST) has issued a statement that says SSL certificates with a key length of 1,024 bits or fewer will be insufficient for security after December 31, 2010, because NIST estimates that computers will be powerful enough to perform a brute-force crack of keys of that size.” BROKEN - Windows IT Pro - Are yur SSL certificates Secure | The research … has the specific purpose of convincing Certification Authorities to drop MD5 and move on to more secure algorithms, such as SHA-1, SHA-2, or the upcoming SHA-3. SoftPedia - SSL Security Broken | .",
            "url": "http://fastpages.ez13.net/wsus/2011/08/12/wsus-best-practices.html",
            "relUrl": "/wsus/2011/08/12/wsus-best-practices.html",
            "date": " • Aug 12, 2011"
        }
        
    
  
    
        ,"post9": {
            "title": "Guidance about WSUS on a Domain Controller",
            "content": "Originally posted on TechNet . Introduction . A common question that comes up during WSUS planning phase is if WSUS is supported on when installed on a Domain Controller. Although this is documented in TechNet in various locations, it is important to highlight some additional recommendations as well as enumerate the main source of documentation for that. The goal of this article is to consolidate the Guidance on running WSUS on a Domain Controller. . Additional Considerations for Domain Controllers and WSUS . The best practice is to not run WSUS on a Domain Controller. “If WSUS is installed a domain controller, this will cause database access issues due to how the database is configured.” This is documented in this best practice article. | “You cannot use a server configured as a domain controller for the back end of the remote SQL pair.” This is documented in the Deployment Guide. | “When you Configure WSUS for Network Load Balancing, none of the servers taking part in the cluster should be a front-end domain controller.” This is documented in the Deployment Guide. | If someone unfamiliar with Domain Controllers is troubleshooting a WSUS and removes the Windows Internal Database, this would be catastrophic to the domain. | It is important to emphasize that by adding another critical role (such as patch management) on the Domain Controller you increase the overall impact in case this server goes offline for any reason. | Separation of server roles is a vital recommendation for high availability scenarios. For this reason, consider server virtualization, where each server role will be in a different virtual machine. | . Thanks to Rob Coffey and Yuri Diogenes for his help with this Article. .",
            "url": "http://fastpages.ez13.net/wsus/2011/08/06/wsus-dc-best-practices.html",
            "relUrl": "/wsus/2011/08/06/wsus-dc-best-practices.html",
            "date": " • Aug 6, 2011"
        }
        
    
  
    
        ,"post10": {
            "title": "Finding machines not compliant with a specific security bulletin",
            "content": "Finding machines not compliant with a specific security bulletin . Originally posted on TechNet. . I read Marc’s BROKEN - post about Compliance Reporting and it was similar to a problem I deal with in my job. Part of my job is to run Update Management on one of the domains consisting of around 12,000 managed computers at Microsoft using WSUS. We do this in order to validate WSUS (and similar products) in an environment with real users. Another group at Microsoft audits my compliance, and often request a list of non-compliant machines for specific security bulletins. I have adapted Marc’s SQL script to do just that. . I ran into one issue, Marc’s SQL script will blocks clients from scanning while it runs. Since the script can take a long time to execute on larger data sets, I decided to allow SQL to read dirty data and unblock my clients (SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED.). . I hope you find this useful. . Travis Plunk . Software Design Engineer in Test II, WSUS . -- Find computers within a target group that need a security bulletin USE SUSDB go SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED GO DECLARE @TargetGroup nvarchar(30) DECLARE @Bulletin nvarchar(9) SELECT @TargetGroup = &#39;All Computers&#39; SELECT @Bulletin = &#39;MS08-030&#39; -- Find the computers not compliant for each security bulletin in the given @TargetGroup -- where the approved occured between @Days and @DaysEnd days ago SELECT ct.Name,@Bulletin as Bulletin,ct.LastReportedStatusTime FROM PUBLIC_VIEWS.vComputerGroupMembership as cgm INNER JOIN PUBLIC_VIEWS.vComputerTarget as ct ON cgm.ComputerTargetId = ct.ComputerTargetId INNER JOIN PUBLIC_VIEWS.vComputerTargetGroup as ctg ON cgm.ComputerTargetGroupId = ctg.ComputerTargetGroupId WHERE (ctg.Name = @TargetGroup) -- And only select those for which an update is approved for install, the -- computer status for that update is either 2 (not installed), 3 (downloaded), -- 5 (failed), or 6 (installed pending reboot), and -- the update bulletin is the one provided. AND EXISTS (SELECT 1 FROM PUBLIC_VIEWS.vUpdateEffectiveApprovalPerComputer as ueapc INNER JOIN PUBLIC_VIEWS.vUpdateApproval as ua ON ua.UpdateApprovalId = ueapc.UpdateApprovalId INNER JOIN PUBLIC_VIEWS.vUpdateInstallationInfoBasic uiib ON uiib.ComputerTargetId = ct.ComputerTargetId AND ua.UpdateId = uiib.UpdateId inner join PUBLIC_VIEWS.vUpdate as u on ua.updateid=u.updateId WHERE (ueapc.ComputerTargetId = ct.ComputerTargetId) AND (ua.Action = &#39;Install&#39;) AND (uiib.State IN (2, 3, 5, 6)) AND u.securityBulletin is not null and u.securityBulletin=@Bulletin ) .",
            "url": "http://fastpages.ez13.net/wsus/2008/07/07/Wsus-reporting.html",
            "relUrl": "/wsus/2008/07/07/Wsus-reporting.html",
            "date": " • Jul 7, 2008"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m Travis Plunk. I’m a Senior Software Engineer at Microsoft. I’ve worked at Microsoft for over 19+ years. I’ve worked on Commerce Server, Windows Update, Windows Server Update Service, PowerShell Desired State Configuration and currently working on PowerShell Core. . Blog Posts | . Work Disclaimer .",
          "url": "http://fastpages.ez13.net/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Work Disclaimer",
          "content": "Disclaimer work makes me put up . The opinions and views expressed in this blog are those of the author and do not necessarily state or reflect those of Microsoft. .",
          "url": "http://fastpages.ez13.net/disclaimer/",
          "relUrl": "/disclaimer/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://fastpages.ez13.net/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}